<!DOCTYPE html>
<html>
<title>W3.CSS Template</title>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" href="https://www.w3schools.com/w3css/4/w3.css">
<link rel="stylesheet" href="style.css">
<body>

<!-- Navbar (sit on top) -->
<div class="top">
  <div class="w3-bar w3-white w3-padding w3-card" style="letter-spacing:4px;">
    <a href="#home" class="w3-bar-item w3-button">Harideep Work Portfolio</a>
    <!-- Right-sided navbar links. Hide them on small screens -->
    <div class="w3-right w3-hide-small">
      <a href="#about" class="w3-bar-item w3-button">About</a>
      <a href="#sprint1" class="w3-bar-item w3-button">Sprint 1</a>
      <a href="#sprint2" class="w3-bar-item w3-button">Sprint 2</a>
      <a href="#sprint3" class="w3-bar-item w3-button">Sprint 3</a>
      <a href="#sprint4" class="w3-bar-item w3-button">Sprint 4</a>
      <a href="#sprint5" class="w3-bar-item w3-button">Sprint 5</a>
      <a href="#sprint6" class="w3-bar-item w3-button">Sprint 6</a>
      <a href="#sprint7" class="w3-bar-item w3-button">Sprint 7</a>
      <a href="#sprint8" class="w3-bar-item w3-button">Sprint 8</a>
    </div>
  </div>
</div>

<!-- Header -->

<section id="hero">
  
  <div class="hero-container">
    <header class="w3-display-container w3-content w3-wide" style="max-width:1600px;min-width:500px;margin-top: 50px;" id="home">
      <!-- <img class="w3-image bcimg"  width="1600" height="800"> -->
      <h1 class="w3-xxlarge" style="color: white; margin-bottom: 2%; padding:5%;">Intern at Multiverz</h1>
      <div class="w3-display-bottomleft w3-padding-large w3-opacity">
        
      </div>
      <div style="margin-top: 0%;">
        <img class="w3-image" src="images1/multiverz-logo-removebg-preview.png" width="200" height="100">
      <h1 style="color: white; font-family: Verdana, Geneva, Tahoma, sans-serif; ">Welcome to Jobverse project</h1>
      <!-- <h3 style="color: white;">We are a team of talented developers working to achieve the target of scraping 1 million job postings</h3> -->
      </div>
      
    </header>

  </div>
  

</section>



<!-- Page content -->
<!-- <div class="w3-content"> -->
<div id="about">
  <div style="margin-left: 15%;margin-right: 15%;margin-top: 30px;">
      <br>
      <br>
      <h1 class="w3-center">About Web scraping</h1><br>
      <p class="w3-large">Initial thoughts on web scraping as a work would be trivial, easy, and unimportant. However, I have realised through the 4 weeks of internship at Multiverse that it is one of the most important steps in the Data science pipeline which most of us turn a blind eye to. </p>
      <p class="w3-large">Structured data is of paramount importance when it comes to data analytics but more than 70% of  data present online is unstructured and hence is of little use. However, Web scraping allows us to “legally” collect information from thousands of websites and structure the data into desirable format which eases further steps of the Data science pipeline.</p>
      <img src="images1/Steps_to_a_Data_Science_Project_Lifecyle.jpg" class="w3-round w3-image " alt="Table Setting">
  </div>
</div>
<br>

<!-- <div class="w3-light-grey"> -->
  <div style="margin-left: 15%;margin-right: 15%;margin-top: 30px;">
      <br>
      <br>
      <h1 class="w3-center">Tools and technologies used</h1><br>
      <!-- <p class="w3-large">Initial thoughts on web scraping as a work would be trivial, easy, and unimportant. However, I have realised through the 4 weeks of internship at Multiverse that it is one of the most important steps in the Data science pipeline which most of us turn a blind eye to. </p> -->
      <!-- <p class="w3-large">Structured data is of paramount importance when it comes to data analytics but more than 70% of  data present online is unstructured and hence is of little use. However, Web scraping allows us to “legally” collect information from thousands of websites and structure the data into desirable format which eases further steps of the Data science pipeline.</p> -->
      <img src="images1/ToolsUsed.png" class="w3-round w3-image " alt="Table Setting">
  </div>
  <br>
<!-- </div> -->

<!-- <div class="w3-light-grey"> -->
<div style="margin-left: 15%;margin-right: 15%;margin-top: 30px; " id="about">
    <br>
    <br>
    <h1 class="w3-center">About Scrapy tool</h1><br>
    <p class="w3-large">There exists a number of ways and libraries to perform the task of web scraping. I have chosen the scrapy tool to do so as it is very fast and supports scalability on an enterprise level. Working of the scrapy tool can be summarized as follows.</p>
    <img src="images1/scrapy_architecture.png" class="w3-round w3-image " alt="Table Setting">
</div>
<br>
<!-- </div> -->

  <!-- About Section -->
  <div class="w3-light-grey">
    <div class="w3-row w3-padding-64" id="sprint1">
      <h1 class="w3-center">Sprint 1</h1><br>
      <div class="w3-col m6 w3-padding-large w3-hide-small">
      <img src="images1/Capture.PNG" class="w3-round w3-image image_left" alt="Table Setting">
      </div>

      <div class="w3-col m6 w3-padding-large">
      <p class="w3-large" style="padding-top:10%">As a part of sprint 1, I researched on how to use the scrapy tool and understood the architecture of the tool. Furthermore, I had scraped all the available 3K Data Scientist job postings from Monster India website to build the skills taxonomy which acted as the starter data for the rest of the teams. I had structured the obtained data into the required pre-defined schema as a JSON format and also converted it into a CSV file as well.</p>
      <!-- <p class="w3-large">Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum consectetur adipiscing elit, sed do eiusmod temporincididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.</p> -->
      </div>
  </div>

<hr>

    
  </div>
  <br>
    
  
  <!-- Menu Section -->
  
    <div class="w3-row w3-padding-64" id="sprint2">
      <h1 class="w3-center">Sprint 2</h1><br>
      <div class="w3-col l6 w3-padding-large">
          <p class="w3-large" style="padding-top:10%">For the sprint 2, I had researched on how the SkillsFuture Singapore website is structured and fetched more than 10K the job posting data from it. Furthermore, I had figured out the architecture of SkillsFuture website where competency levels for both the technical and generic skills for each job role is given. Obtained the required skills competency levels data for each job role under different sectors and structured it into CSV file. To reduce the manual effort of going through the pdf’s to scrape the data, I had proactively written a script to automate the pdf’s scraping which saved many hours of tedious manual work.</p>
      </div>
      
      <div class="w3-col l6 w3-padding-large">
        <img src="images1/SkillsFuture.png" class="w3-round w3-image image_right" alt="Menu">
      </div>
    </div>
  </div>
  <hr>

  <!-- About Section -->
  <div class="w3-light-grey">
    <div class="w3-row w3-padding-64" id="sprint3">
        <h1 class="w3-center">Sprint 3</h1><br>
        <div class="w3-col m6 w3-padding-large w3-hide-small">
        <img src="images1/Amazon.PNG" class="w3-round w3-image image_left" alt="Table Setting">
        </div>

        <div class="w3-col m6 w3-padding-large">
        <p class="w3-large" style="padding-top:10%">As part of the sprint 3, I started scraping job posting data of current openings in the Fortune top 500 companies. As I had worked on the job aggregator websites such as Monster India in the previous sprints where the content was relatively structured, scraping was comparatively easier than in the case of company websites which are usually different from each other. I had scraped all the available 9K job postings from the Amazon company website which stands 2rd in Fortune 500 companies and processed the obtained data into predefined schema and structured it into a CSV file.</p>
     
        </div>
    </div>
  </div>
  <br>

  <hr>

  <!-- Menu Section -->
  
  <div class="w3-row w3-padding-64" id="sprint4">
    <h1 class="w3-center">Sprint 4</h1><br>
    <div class="w3-col l6 w3-padding-large">
        <p class="w3-large" style="padding-top:20%">In the sprint 4, I continued with scrapping companies in the Fortune list. I had scrapped more than 7K job postings from Apple and Verizon communications and converted them into required unified format to store in a csv file. Fortunately, I didn’t encounter any major issues and was able to scrape the data using the scrapy tool.</p>  
    </div>
    
    <div class="w3-col l6 w3-padding-large">
      <img src="images1/Apple.PNG" class="w3-round w3-image image_right" alt="Menu">
    </div>
  </div>

  <hr>

  <!-- About Section -->
  <div class="w3-light-grey"> 
    <div class="w3-row w3-padding-64" id="sprint5" >
        <h1 class="w3-center">Sprint 5</h1><br>
        <div class="w3-col m6 w3-padding-large w3-hide-small">
        <img src="images1/Fortune200.PNG" class="w3-round w3-image image_left">
        </div>

        <div class="w3-col m6 w3-padding-large">
        <p class="w3-large" style="padding-top:10%">As part of the sprint 5, I had started with research on how to increase the speed of scraping using the scrapy tool to reach the given targets. I had implemented a lighter version of middleware in the scrapy architecture and utilised a few other miniature techniques to improve the speed of the scrapy by more than twice. With this powerful update I was able to scrape more than 6 Fortune 500 companies and scraped more than 20K  job postings. Furthermore, I had also scraped a website using the Selenium tool as the website didn’t allow the scrapy tool. I observed that scrapy is much faster than selenium as it mimicked the human form of browsing.</p>
        </div>
    </div>
  </div>
  <br>



    <!-- About Sprint -6 -->
    
      <div class="w3-row w3-padding-64" id="sprint6">
        <h1 class="w3-center">Sprint 6</h1><br>
        <div class="w3-col l6 w3-padding-large">
            <p class="w3-large" style="padding-top:10%">As part of the sprint 6, since there were only a few company sites in the fortune 500 list left with a high number of job postings and that can be scraped using scrapy, I started the sprint with a thorough research of the rest of the websites to prioritise few websites for this sprint. I went ahead with Bank of America, Chipotle, Stryker, Murphy USA, and Cognizant websites. In this sprint, I scraped a total of 10,000 job postings. Because of the aria-hidden tags in Cognizant, I had to switch to selenium to scrape the job data. A total of 2,000 more from Cognizant were retrieved. All 12,000 job postings were stored in the form of CSV as per the unified schema defined.</p>  
        </div>
        
        <div class="w3-col l6 w3-padding-large">
          <img src="images1/sprint-6.png" class="w3-round w3-image image_right" alt="Menu">
        </div>
      </div>
    
  
  <!-- About Section -->
  <div class="w3-light-grey">
    <div class="w3-row w3-padding-64" id="sprint7" >
      <h1 class="w3-center">Sprint 7</h1><br>
      <div class="w3-col m6 w3-padding-large w3-hide-small">
      <img src="images1/sprint-7.png" class="w3-round w3-image image_left">
      </div>
  
      <div class="w3-col m6 w3-padding-large">
      <p class="w3-large" style="padding-top:10%"> In the sprint 7, in order to achieve the target of scraping 1 million job postings I started the sprint with an extensive search for a job aggregator website with access to huge number of job postings. Fortunately, I found Career builder site with access to more than 5 lakh job postings. However, I had this major issue of duplication as in almost 50% of the job postings were repetitive, leaving with 2.5 lakh unique job postings. As soon as the scraping of Career builder website was done, I took up the task of preparing a basic outline architecture on AWS to automate the scraping process. I created an AWS account and a lightsail Linux instance in it to do the job. Assuming that a lightsail instance would be allocated to scrape a website, I worked towards creating a working demo of basic automation process. AWS services such as Lightsail instances, Lambda functions, S3 bucket, Api Gateway, and Cloud watch along with Github were utilised to achieve the task of autamtion.
       </p>
      </div>
  </div>
  </div>
  <br>
  


<!-- About Sprint -8 -->

  <div class="w3-row w3-padding-64" id="sprint8">
    <h1 class="w3-center">Sprint 8</h1><br>
    <div class="w3-col l6 w3-padding-large">
        <p class="w3-large" style="padding-top:5%">As part of sprint 8, as reaching our target of 1 million job postings are prioritised, I again started looking for a job aggregator site with access to huge number of job postings. I found a website known as CV library which gave access to almost 1.5 lakh job postings with almost all the required attributes as per the unified schema. As I was able to scrape the website using the scrapy tool without any middlewares, I was able to scrape the website in a much faster way. I completed scraping 1.45 lakh job postings and started with the JobisJob aggregator site. so far, I scrapped more than 20000 job postings from the JobisJob website.</p>  
    </div>
    
    <div class="w3-col l6 w3-padding-large">
      <img src="images1/Workprogress.png" class="w3-round w3-image image_right" alt="Menu">
    </div>
  </div>
</div>

<!-- End page content -->
<!-- </div> -->


</body>
</html>
